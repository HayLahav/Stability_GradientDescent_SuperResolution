{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaFM Minimax Tutorial\n",
    "\n",
    "Interactive tutorial on the AdaFM optimizer for minimax optimization problems.\n",
    "\n",
    "Based on: \"AdaFM: Adaptive Variance-Reduced Algorithm for Stochastic Minimax Optimization\" (ICLR 2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "from src.optimizers import AdaFMOptimizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Minimax Problems\n",
    "\n",
    "Minimax problems have the form:\n",
    "$$\\min_x \\max_y f(x, y)$$\n",
    "\n",
    "Common in:\n",
    "- GANs (Generator vs Discriminator)\n",
    "- Robust optimization\n",
    "- Game theory\n",
    "- Adversarial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Simple bilinear problem\n",
    "def bilinear_problem(x, y, A=None):\n",
    "    \"\"\"\n",
    "    f(x,y) = x^T A y + ||x||^2/2 - ||y||^2/2\n",
    "    \"\"\"\n",
    "    if A is None:\n",
    "        A = torch.tensor([[1.0, 0.5], [0.5, 1.0]])\n",
    "    \n",
    "    bilinear = torch.dot(x, A @ y)\n",
    "    reg_x = 0.5 * torch.norm(x)**2\n",
    "    reg_y = 0.5 * torch.norm(y)**2\n",
    "    \n",
    "    return bilinear + reg_x - reg_y\n",
    "\n",
    "# Visualize the landscape\n",
    "x_range = np.linspace(-3, 3, 50)\n",
    "y_range = np.linspace(-3, 3, 50)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "\n",
    "# Compute function values\n",
    "Z = np.zeros_like(X)\n",
    "for i in range(len(x_range)):\n",
    "    for j in range(len(y_range)):\n",
    "        x = torch.tensor([X[i,j], 0.0])\n",
    "        y = torch.tensor([Y[i,j], 0.0])\n",
    "        Z[i,j] = bilinear_problem(x, y).item()\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 3D surface\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "surf = ax1.plot_surface(X, Y, Z, cmap=cm.coolwarm, alpha=0.8)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_zlabel('f(x,y)')\n",
    "ax1.set_title('Bilinear Minimax Problem')\n",
    "\n",
    "# Contour plot\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contour(X, Y, Z, levels=20)\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_title('Contour Plot')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AdaFM Algorithm Overview\n",
    "\n",
    "AdaFM uses:\n",
    "1. **Filtered momentum**: $v_t = \\nabla_x f(x_t, y_t) + (1-\\beta_t)(v_{t-1} - \\nabla_x f(x_{t-1}, y_{t-1}))$\n",
    "2. **Adaptive learning rates**: $\\eta_x = \\gamma / \\alpha_x^{1/3+\\delta}$, $\\eta_y = \\lambda / \\alpha_y^{1/3-\\delta}$\n",
    "3. **Momentum schedule**: $\\beta_t = 1/t^{2/3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement AdaFM step-by-step\n",
    "class MinimaxProblem:\n",
    "    def __init__(self, dim=10):\n",
    "        self.dim = dim\n",
    "        # Random matrix for interesting dynamics\n",
    "        self.A = torch.randn(dim, dim) * 0.3\n",
    "        self.A = (self.A + self.A.T) / 2  # Make symmetric\n",
    "        \n",
    "    def f(self, x, y):\n",
    "        bilinear = torch.dot(x, self.A @ y)\n",
    "        reg_x = 0.5 * torch.norm(x)**2\n",
    "        reg_y = 0.5 * torch.norm(y)**2\n",
    "        return bilinear + reg_x - reg_y\n",
    "    \n",
    "    def grad_x(self, x, y):\n",
    "        return self.A @ y + x\n",
    "    \n",
    "    def grad_y(self, x, y):\n",
    "        return self.A.T @ x - y\n",
    "\n",
    "# Compare optimizers\n",
    "def compare_optimizers(problem, num_iterations=200):\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Standard GDA\n",
    "    x_gda = torch.randn(problem.dim, requires_grad=True)\n",
    "    y_gda = torch.randn(problem.dim, requires_grad=True)\n",
    "    opt_x_gda = torch.optim.SGD([x_gda], lr=0.01)\n",
    "    opt_y_gda = torch.optim.SGD([y_gda], lr=0.02)\n",
    "    \n",
    "    history_gda = {'f_vals': [], 'grad_norms': []}\n",
    "    \n",
    "    for t in range(num_iterations):\n",
    "        # Compute gradients\n",
    "        x_gda.grad = problem.grad_x(x_gda, y_gda)\n",
    "        y_gda.grad = -problem.grad_y(x_gda, y_gda)\n",
    "        \n",
    "        # Record metrics\n",
    "        f_val = problem.f(x_gda, y_gda).item()\n",
    "        grad_norm = torch.norm(x_gda.grad).item() + torch.norm(y_gda.grad).item()\n",
    "        history_gda['f_vals'].append(f_val)\n",
    "        history_gda['grad_norms'].append(grad_norm)\n",
    "        \n",
    "        # Update\n",
    "        opt_x_gda.step()\n",
    "        opt_y_gda.step()\n",
    "        opt_x_gda.zero_grad()\n",
    "        opt_y_gda.zero_grad()\n",
    "    \n",
    "    results['GDA'] = history_gda\n",
    "    \n",
    "    # 2. AdaFM\n",
    "    x_adafm = torch.randn(problem.dim, requires_grad=True)\n",
    "    y_adafm = torch.randn(problem.dim, requires_grad=True)\n",
    "    optimizer_adafm = AdaFMOptimizer(\n",
    "        params_x=[x_adafm],\n",
    "        params_y=[y_adafm],\n",
    "        gamma=1.0,\n",
    "        lam=1.0,\n",
    "        delta=0.001,\n",
    "        single_variable=False\n",
    "    )\n",
    "    \n",
    "    history_adafm = {'f_vals': [], 'grad_norms': [], 'lrs': []}\n",
    "    \n",
    "    for t in range(num_iterations):\n",
    "        # Compute gradients\n",
    "        x_adafm.grad = problem.grad_x(x_adafm, y_adafm)\n",
    "        y_adafm.grad = -problem.grad_y(x_adafm, y_adafm)\n",
    "        \n",
    "        # Record metrics\n",
    "        f_val = problem.f(x_adafm, y_adafm).item()\n",
    "        grad_norm = torch.norm(x_adafm.grad).item() + torch.norm(y_adafm.grad).item()\n",
    "        history_adafm['f_vals'].append(f_val)\n",
    "        history_adafm['grad_norms'].append(grad_norm)\n",
    "        \n",
    "        # Get current learning rates\n",
    "        lrs = optimizer_adafm.get_current_lrs()\n",
    "        history_adafm['lrs'].append(lrs)\n",
    "        \n",
    "        # Update\n",
    "        optimizer_adafm.step()\n",
    "        optimizer_adafm.zero_grad()\n",
    "    \n",
    "    results['AdaFM'] = history_adafm\n",
    "    \n",
    "    return results\n",
    "\n