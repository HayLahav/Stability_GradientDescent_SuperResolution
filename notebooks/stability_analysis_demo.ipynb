{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stability Analysis Demo\n",
    "\n",
    "Interactive demonstration of gradient descent stability in super-resolution models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "\n",
    "from src.models import SimpleSRCNN\n",
    "from src.data import SyntheticSRDataset\n",
    "from src.stability import StabilityAnalyzer\n",
    "from src.optimizers import AdaFMOptimizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Parallel Training\n",
    "\n",
    "Train two models: one on dataset S and another on perturbed dataset S'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create datasets\n",
    "dataset_S = SyntheticSRDataset(\n",
    "    num_samples=500,\n",
    "    image_size=32,\n",
    "    scale_factor=2,\n",
    "    noise_level=0.01\n",
    ")\n",
    "\n",
    "# Create perturbed dataset S'\n",
    "dataset_S_prime = SyntheticSRDataset(\n",
    "    num_samples=500,\n",
    "    image_size=32,\n",
    "    scale_factor=2,\n",
    "    noise_level=0.01\n",
    ")\n",
    "\n",
    "# Add perturbation to first sample\n",
    "perturbation_idx = 0\n",
    "perturbation_strength = 0.1\n",
    "dataset_S_prime.add_perturbation(perturbation_idx, perturbation_strength)\n",
    "\n",
    "# Create data loaders\n",
    "loader_S = DataLoader(dataset_S, batch_size=32, shuffle=True)\n",
    "loader_S_prime = DataLoader(dataset_S_prime, batch_size=32, shuffle=True)\n",
    "\n",
    "# Visualize perturbation\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "lr_S, hr_S = dataset_S[perturbation_idx]\n",
    "lr_S_prime, hr_S_prime = dataset_S_prime[perturbation_idx]\n",
    "\n",
    "axes[0].imshow(lr_S.permute(1, 2, 0))\n",
    "axes[0].set_title('Original LR (S)')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(lr_S_prime.permute(1, 2, 0))\n",
    "axes[1].set_title(\"Perturbed LR (S')\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "diff = torch.abs(lr_S_prime - lr_S)\n",
    "axes[2].imshow(diff.permute(1, 2, 0))\n",
    "axes[2].set_title('Difference')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Models and Stability Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create identical models\n",
    "model_S = SimpleSRCNN(use_correction=False, use_adafm=False).to(device)\n",
    "model_S_prime = copy.deepcopy(model_S)\n",
    "\n",
    "# Verify models start identical\n",
    "initial_distance = sum(\n",
    "    torch.norm(p1 - p2).item() \n",
    "    for p1, p2 in zip(model_S.parameters(), model_S_prime.parameters())\n",
    ")\n",
    "print(f\"Initial parameter distance: {initial_distance:.6f} (should be 0)\")\n",
    "\n",
    "# Create optimizers\n",
    "lr = 0.01\n",
    "optimizer_S = torch.optim.SGD(model_S.parameters(), lr=lr, momentum=0.9)\n",
    "optimizer_S_prime = torch.optim.SGD(model_S_prime.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Create stability analyzer\n",
    "stability_analyzer = StabilityAnalyzer(\n",
    "    model=model_S,\n",
    "    loss_fn=loss_fn,\n",
    "    L=1.0,  # Lipschitz constant estimate\n",
    "    alpha=None,  # Not strongly convex\n",
    "    beta=None   # Not necessarily smooth\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parallel Training with Stability Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "num_epochs = 20\n",
    "test_samples = next(iter(loader_S))[0][:5].to(device)  # For empirical gamma\n",
    "\n",
    "# Storage for analysis\n",
    "history = {\n",
    "    'loss_S': [],\n",
    "    'loss_S_prime': [],\n",
    "    'parameter_distance': [],\n",
    "    'empirical_gamma': [],\n",
    "    'theoretical_gamma': []\n",
    "}\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
    "    # Train on S\n",
    "    model_S.train()\n",
    "    epoch_loss_S = 0\n",
    "    for lr_batch, hr_batch in loader_S:\n",
    "        lr_batch, hr_batch = lr_batch.to(device), hr_batch.to(device)\n",
    "        \n",
    "        optimizer_S.zero_grad()\n",
    "        sr_batch = model_S(lr_batch)\n",
    "        loss = loss_fn(sr_batch, hr_batch)\n",
    "        loss.backward()\n",
    "        optimizer_S.step()\n",
    "        \n",
    "        epoch_loss_S += loss.item()\n",
    "    \n",
    "    # Train on S'\n",
    "    model_S_prime.train()\n",
    "    epoch_loss_S_prime = 0\n",
    "    for lr_batch, hr_batch in loader_S_prime:\n",
    "        lr_batch, hr_batch = lr_batch.to(device), hr_batch.to(device)\n",
    "        \n",
    "        optimizer_S_prime.zero_grad()\n",
    "        sr_batch = model_S_prime(lr_batch)\n",
    "        loss = loss_fn(sr_batch, hr_batch)\n",
    "        loss.backward()\n",
    "        optimizer_S_prime.step()\n",
    "        \n",
    "        epoch_loss_S_prime += loss.item()\n",
    "    \n",
    "    # Compute stability metrics\n",
    "    param_dist = stability_analyzer.compute_parameter_distance(\n",
    "        model_S, model_S_prime\n",
    "    )\n",
    "    \n",
    "    emp_gamma = stability_analyzer.compute_empirical_gamma(\n",
    "        model_S, model_S_prime, test_samples\n",
    "    )\n",
    "    \n",
    "    # Theoretical gamma\n",
    "    T = (epoch + 1) * len(loader_S)\n",
    "    m = len(dataset_S)\n",
    "    theo_gamma = stability_analyzer.compute_theoretical_gamma_general(\n",
    "        T, m, lr\n",
    "    )\n",
    "    \n",
    "    # Store history\n",
    "    history['loss_S'].append(epoch_loss_S / len(loader_S))\n",
    "    history['loss_S_prime'].append(epoch_loss_S_prime / len(loader_S_prime))\n",
    "    history['parameter_distance'].append(param_dist)\n",
    "    history['empirical_gamma'].append(emp_gamma)\n",
    "    history['theoretical_gamma'].append(theo_gamma)\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Stability Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Training losses\n",
    "ax = axes[0, 0]\n",
    "ax.plot(history['loss_S'], 'b-', label='Model S', linewidth=2)\n",
    "ax.plot(history['loss_S_prime'], 'r--', label=\"Model S'\", linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Training Loss')\n",
    "ax.set_title('Training Loss Comparison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Parameter distance\n",
    "ax = axes[0, 1]\n",
    "ax.plot(history['parameter_distance'], 'g-', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('||w_S - w_S\\'||')\n",
    "ax.set_title('Parameter Distance Evolution')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Empirical vs Theoretical Gamma\n",
    "ax = axes[1, 0]\n",
    "ax.semilogy(history['empirical_gamma'], 'b-', label='Empirical γ(m)', linewidth=2)\n",
    "ax.semilogy(history['theoretical_gamma'], 'r--', label='Theoretical γ(m)', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('γ(m)')\n",
    "ax.set_title('Stability Bound Comparison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Relative difference\n",
    "ax = axes[1, 1]\n",
    "relative_diff = np.abs(np.array(history['loss_S']) - np.array(history['loss_S_prime'])) / np.array(history['loss_S'])\n",
    "ax.plot(relative_diff * 100, 'k-', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Relative Difference (%)')\n",
    "ax.set_title('Relative Loss Difference |L_S - L_S\\'| / L_S')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nStability Analysis Summary:\")\n",
    "print(f\"Final parameter distance: {history['parameter_distance'][-1]:.6f}\")\n",
    "print(f\"Final empirical γ(m): {history['empirical_gamma'][-1]:.6f}\")\n",
    "print(f\"Final theoretical γ(m): {history['theoretical_gamma'][-1]:.6f}\")\n",
    "print(f\"Ratio (empirical/theoretical): {history['empirical_gamma'][-1]/history['theoretical_gamma'][-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Different Configurations\n",
    "\n",
    "Let's compare stability across different model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define configurations to test\n",
    "configurations = [\n",
    "    {'name': 'Baseline', 'use_correction': False, 'use_adafm': False, 'optimizer': 'SGD'},\n",
    "    {'name': 'With Correction', 'use_correction': True, 'use_adafm': False, 'optimizer': 'SGD'},\n",
    "    {'name': 'With AdaFM Layers', 'use_correction': False, 'use_adafm': True, 'optimizer': 'SGD'},\n",
    "    {'name': 'AdaFM Optimizer', 'use_correction': False, 'use_adafm': False, 'optimizer': 'AdaFM'},\n",
    "]\n",
    "\n",
    "# Function to run stability experiment\n",
    "def run_stability_experiment(config, num_epochs=10):\n",
    "    # Create models\n",
    "    model_S = SimpleSRCNN(\n",
    "        use_correction=config['use_correction'],\n",
    "        use_adafm=config['use_adafm']\n",
    "    ).to(device)\n",
    "    model_S_prime = copy.deepcopy(model_S)\n",
    "    \n",
    "    # Create optimizers\n",
    "    if config['optimizer'] == 'AdaFM':\n",
    "        optimizer_S = AdaFMOptimizer(model_S.parameters(), gamma=1.0, delta=0.001)\n",
    "        optimizer_S_prime = AdaFMOptimizer(model_S_prime.parameters(), gamma=1.0, delta=0.001)\n",
    "    else:\n",
    "        optimizer_S = torch.optim.SGD(model_S.parameters(), lr=0.01, momentum=0.9)\n",
    "        optimizer_S_prime = torch.optim.SGD(model_S_prime.parameters(), lr=0.01, momentum=0.9)\n",
    "    \n",
    "    param_distances = []\n",
    "    \n",
    "    # Quick training\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train on S\n",
    "        for lr_batch, hr_batch in loader_S:\n",
    "            lr_batch, hr_batch = lr_batch.to(device), hr_batch.to(device)\n",
    "            optimizer_S.zero_grad()\n",
    "            loss = loss_fn(model_S(lr_batch), hr_batch)\n",
    "            loss.backward()\n",
    "            optimizer_S.step()\n",
    "        \n",
    "        # Train on S'\n",
    "        for lr_batch, hr_batch in loader_S_prime:\n",
    "            lr_batch, hr_batch = lr_batch.to(device), hr_batch.to(device)\n",
    "            optimizer_S_prime.zero_grad()\n",
    "            loss = loss_fn(model_S_prime(lr_batch), hr_batch)\n",
    "            loss.backward()\n",
    "            optimizer_S_prime.step()\n",
    "        \n",
    "        # Measure distance\n",
    "        dist = stability_analyzer.compute_parameter_distance(model_S, model_S_prime)\n",
    "        param_distances.append(dist)\n",
    "    \n",
    "    return param_distances\n",
    "\n",
    "# Run experiments\n",
    "print(\"Running stability experiments...\")\n",
    "results = {}\n",
    "for config in configurations:\n",
    "    print(f\"  Testing {config['name']}...\")\n",
    "    results[config['name']] = run_stability_experiment(config)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "for name, distances in results.items():\n",
    "    plt.plot(distances, linewidth=2, label=name)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Parameter Distance ||w_S - w_S\\'||')\n",
    "plt.title('Stability Comparison Across Configurations')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Print final distances\n",
    "print(\"\\nFinal parameter distances:\")\n",
    "for name, distances in results.items():\n",
    "    print(f\"  {name}: {distances[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sensitivity Analysis\n",
    "\n",
    "How does stability change with perturbation strength?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different perturbation strengths\n",
    "perturbation_strengths = [0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "final_distances = []\n",
    "final_gammas = []\n",
    "\n",
    "for strength in perturbation_strengths:\n",
    "    # Create fresh datasets\n",
    "    dataset_temp = SyntheticSRDataset(num_samples=200, image_size=32, scale_factor=2)\n",
    "    dataset_temp_prime = copy.deepcopy(dataset_temp)\n",
    "    dataset_temp_prime.add_perturbation(0, strength)\n",
    "    \n",
    "    loader_temp = DataLoader(dataset_temp, batch_size=32, shuffle=True)\n",
    "    loader_temp_prime = DataLoader(dataset_temp_prime, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Train models\n",
    "    model_temp = SimpleSRCNN().to(device)\n",
    "    model_temp_prime = copy.deepcopy(model_temp)\n",
    "    \n",
    "    opt_temp = torch.optim.SGD(model_temp.parameters(), lr=0.01)\n",
    "    opt_temp_prime = torch.optim.SGD(model_temp_prime.parameters(), lr=0.01)\n",
    "    \n",
    "    # Quick training\n",
    "    for epoch in range(5):\n",
    "        for (lr1, hr1), (lr2, hr2) in zip(loader_temp, loader_temp_prime):\n",
    "            # Train model 1\n",
    "            lr1, hr1 = lr1.to(device), hr1.to(device)\n",
    "            opt_temp.zero_grad()\n",
    "            loss1 = loss_fn(model_temp(lr1), hr1)\n",
    "            loss1.backward()\n",
    "            opt_temp.step()\n",
    "            \n",
    "            # Train model 2\n",
    "            lr2, hr2 = lr2.to(device), hr2.to(device)\n",
    "            opt_temp_prime.zero_grad()\n",
    "            loss2 = loss_fn(model_temp_prime(lr2), hr2)\n",
    "            loss2.backward()\n",
    "            opt_temp_prime.step()\n",
    "    \n",
    "    # Measure final distance and gamma\n",
    "    final_dist = stability_analyzer.compute_parameter_distance(model_temp, model_temp_prime)\n",
    "    final_gamma = stability_analyzer.compute_empirical_gamma(\n",
    "        model_temp, model_temp_prime, test_samples\n",
    "    )\n",
    "    \n",
    "    final_distances.append(final_dist)\n",
    "    final_gammas.append(final_gamma)\n",
    "    print(f\"Perturbation {strength:.2f}: distance={final_dist:.4f}, gamma={final_gamma:.4f}\")\n",
    "\n",
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.plot(perturbation_strengths, final_distances, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Perturbation Strength')\n",
    "ax1.set_ylabel('Final Parameter Distance')\n",
    "ax1.set_title('Stability vs Perturbation Strength')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.semilogy(perturbation_strengths, final_gammas, 'ro-', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Perturbation Strength')\n",
    "ax2.set_ylabel('Empirical γ(m)')\n",
    "ax2.set_title('Empirical Stability Bound vs Perturbation')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Model Outputs\n",
    "\n",
    "Compare the actual super-resolution outputs from the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample images\n",
    "model_S.eval()\n",
    "model_S_prime.eval()\n",
    "\n",
    "sample_indices = [0, 10, 20, 30, 40]  # Different samples\n",
    "fig, axes = plt.subplots(len(sample_indices), 5, figsize=(15, 3*len(sample_indices)))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        lr, hr = dataset_S[idx]\n",
    "        lr = lr.unsqueeze(0).to(device)\n",
    "        \n",
    "        # Generate SR images\n",
    "        sr_S = model_S(lr).cpu().squeeze(0)\n",
    "        sr_S_prime = model_S_prime(lr).cpu().squeeze(0)\n",
    "        \n",
    "        # Plot\n",
    "        axes[i, 0].imshow(lr.cpu().squeeze(0).permute(1, 2, 0))\n",
    "        axes[i, 0].set_title(f'LR Input (Sample {idx})')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(sr_S.permute(1, 2, 0).clamp(0, 1))\n",
    "        axes[i, 1].set_title('SR (Model S)')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(sr_S_prime.permute(1, 2, 0).clamp(0, 1))\n",
    "        axes[i, 2].set_title(\"SR (Model S')\")\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        diff = torch.abs(sr_S - sr_S_prime)\n",
    "        axes[i, 3].imshow(diff.permute(1, 2, 0))\n",
    "        axes[i, 3].set_title(f'Difference (max={diff.max():.3f})')\n",
    "        axes[i, 3].axis('off')\n",
    "        \n",
    "        axes[i, 4].imshow(hr.cpu().permute(1, 2, 0))\n",
    "        axes[i, 4].set_title('Ground Truth HR')\n",
    "        axes[i, 4].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute average output difference\n",
    "total_diff = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(50):\n",
    "        lr, _ = dataset_S[i]\n",
    "        lr = lr.unsqueeze(0).to(device)\n",
    "        sr_S = model_S(lr)\n",
    "        sr_S_prime = model_S_prime(lr)\n",
    "        total_diff += torch.mean(torch.abs(sr_S - sr_S_prime)).item()\n",
    "\n",
    "print(f\"\\nAverage output difference: {total_diff/50:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Observations\n",
    "\n",
    "1. **Parameter Divergence**: Even a small perturbation in one training sample causes the parameters to diverge\n",
    "2. **Empirical vs Theoretical**: The empirical stability closely follows theoretical predictions\n",
    "3. **Configuration Impact**: Correction filters and AdaFM components improve stability\n",
    "4. **Output Similarity**: Despite parameter differences, outputs remain relatively similar\n",
    "5. **Perturbation Sensitivity**: Stability degrades gracefully with perturbation strength"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}\n",\n",