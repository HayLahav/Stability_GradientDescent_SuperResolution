{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{ “cells”: \\[ { “cell_type”: “markdown”, “metadata”: {}, “source”: \\[\n",
    "“\\# Theoretical Validation of Stability Bounds”, “”, “This notebook\n",
    "validates the theoretical stability bounds from Lecture 8 on Stability\n",
    "Analysis for Gradient Descent.”, “”, “We’ll verify:”, “1. Strongly\n",
    "Convex Case (Theorem 8.1)”, “2. General Case (Theorem 8.3)”, “3. Smooth\n",
    "Case (Theorem 8.5)”, “4. Price of Stability Analysis” \\] }, {\n",
    "“cell_type”: “code”, “execution_count”: null, “metadata”: {}, “outputs”:\n",
    "\\[\\], “source”: \\[ “\\# Setup”, “import sys”, “sys.path.append(‘..’)”,\n",
    "“”, “import numpy as np”, “import matplotlib.pyplot as plt”, “import\n",
    "torch”, “from src.stability.theoretical_bounds import (”, ”\n",
    "compute_strongly_convex_bound,“,” compute_general_bound,“,”\n",
    "compute_smooth_bound,“,” analyze_price_of_stability“,”)“,”“,”\\#\n",
    "Configure\n",
    "plotting“,”plt.style.use(‘seaborn-v0_8-darkgrid’)“,”%matplotlib inline”\n",
    "\\] }, { “cell_type”: “markdown”, “metadata”: {}, “source”: \\[ “\\## 1.\n",
    "Strongly Convex Case (Theorem 8.1)”, “”, “For strongly convex functions\n",
    "with parameter α:”,\n",
    "“$$\\\\gamma(m) = O\\\\left(\\\\frac{L^2}{\\\\alpha\\\\sqrt{T}} + \\\\frac{L^2}{\\\\alpha m}\\\\right)$$”\n",
    "\\] }, { “cell_type”: “code”, “execution_count”: null, “metadata”: {},\n",
    "“outputs”: \\[\\], “source”: \\[ “\\# Parameters”, “L = 1.0 \\# Lipschitz\n",
    "constant”, “alpha = 0.1 \\# Strong convexity parameter”, “m = 1000 \\#\n",
    "Sample size”, “T_values = np.logspace(1, 5, 100) \\# Iterations from 10\n",
    "to 100,000”, “”, “\\# Compute bounds”, “gamma_sc =\n",
    "compute_strongly_convex_bound(T_values, m, 1.0, L, alpha)”, “”, “\\#\n",
    "Plot”, “plt.figure(figsize=(10, 6))”, “plt.loglog(T_values, gamma_sc,\n",
    "‘b-’, linewidth=2, label=‘Stability Bound γ(m)’)”, “plt.loglog(T_values,\n",
    "L\\*\\*2/(alpha\\*np.sqrt(T_values)), ‘r–’, “,” label=‘First term:\n",
    "$L^2/(α√T)$’)“,”plt.loglog(T_values,\n",
    "L**2/(alpha*m)*np.ones_like(T_values), ‘g–’, “,” label=‘Second term:\n",
    "$L^2/(αm)$’)“,”plt.xlabel(‘Number of Iterations\n",
    "(T)’)“,”plt.ylabel(‘Stability Bound’)“,”plt.title(‘Strongly Convex Case:\n",
    "Stability Bound Decomposition’)“,”plt.legend()“,”plt.grid(True,\n",
    "alpha=0.3)“,”plt.show()“,”“,”\\# Find crossover point“,”crossover_T =\n",
    "m**2“,”print(f\"Crossover point: T = {crossover_T:,}\")“,”print(f\"Below T\n",
    "= {crossover_T:,}, the 1/√T term dominates\")“,”print(f\"Above T =\n",
    "{crossover_T:,}, the 1/m term dominates\")” \\] }, { “cell_type”:\n",
    "“markdown”, “metadata”: {}, “source”: \\[ “\\## 2. General Case (Theorem\n",
    "8.3)”, “”, “For general convex functions:”,\n",
    "“$$\\\\gamma(m) = 4\\\\eta L^2\\\\sqrt{T} + \\\\frac{4\\\\eta L^2 T}{m}$$” \\] }, {\n",
    "“cell_type”: “code”, “execution_count”: null, “metadata”: {}, “outputs”:\n",
    "\\[\\], “source”: \\[ “\\# Test different learning rates”, “eta_values =\n",
    "\\[0.1, 0.01, 0.001\\]”, “colors = \\[‘blue’, ‘red’, ‘green’\\]”, “”,\n",
    "“plt.figure(figsize=(12, 8))”, “”, “for eta, color in zip(eta_values,\n",
    "colors):”, ” gamma_general = compute_general_bound(T_values, m, eta,\n",
    "L)“,” plt.loglog(T_values, gamma_general, f’{color\\[0\\]}-‘, “,”\n",
    "linewidth=2, label=f’η = {eta}’)“,” “,” \\# Show optimal learning rate“,”\n",
    "eta_opt = np.sqrt(m) / (L \\* np.sqrt(T_values))“,” gamma_opt =\n",
    "compute_general_bound(T_values, m, eta_opt, L)“,”\n",
    "“,”plt.loglog(T_values, gamma_opt, ‘k–’, linewidth=2, “,” label=‘Optimal\n",
    "η(T) = √m/(L√T)’)“,”“,”plt.xlabel(‘Number of Iterations\n",
    "(T)’)“,”plt.ylabel(‘Stability Bound γ(m)’)“,”plt.title(‘General Case:\n",
    "Effect of Learning Rate on Stability’)“,”plt.legend()“,”plt.grid(True,\n",
    "alpha=0.3)“,”plt.show()“,”“,”\\# Analyze optimal learning rate“,”T_test =\n",
    "10000“,”eta_optimal = np.sqrt(m) / (L \\* np.sqrt(T_test))“,”print(f\"For\n",
    "T = {T_test:,} and m = {m}:\")“,”print(f\"Optimal learning rate: η\\* =\n",
    "{eta_optimal:.6f}\")“,”print(f\"This gives γ(m) = O(L²/√m)\")” \\] }, {\n",
    "“cell_type”: “markdown”, “metadata”: {}, “source”: \\[ “\\## 3. Smooth\n",
    "Case (Theorem 8.5)”, “”, “For β-smooth functions:”,\n",
    "“$$\\\\gamma(m) = \\\\frac{2\\\\eta T L^2}{m}$$” \\] }, { “cell_type”: “code”,\n",
    "“execution_count”: null, “metadata”: {}, “outputs”: \\[\\], “source”: \\[\n",
    "“\\# Parameters for smooth case”, “beta = 1.0 \\# Smoothness parameter”,\n",
    "“eta_smooth = 1.0 / beta \\# Optimal for smooth case”, “”, “\\# Compare\n",
    "all three cases”, “fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,\n",
    "6))”, “”, “\\# Left plot: Fixed m, varying T”, “gamma_sc =\n",
    "compute_strongly_convex_bound(T_values, m, 1.0, L, alpha)”, “gamma_gen =\n",
    "compute_general_bound(T_values, m, np.sqrt(m)/(L\\*np.sqrt(T_values)),\n",
    "L)“,”gamma_smooth = compute_smooth_bound(T_values, m, eta_smooth, L,\n",
    "beta)“,”“,”ax1.loglog(T_values, gamma_sc, ‘b-’, linewidth=2,\n",
    "label=‘Strongly Convex’)“,”ax1.loglog(T_values, gamma_gen, ‘r-’,\n",
    "linewidth=2, label=‘General Case’)“,”ax1.loglog(T_values, gamma_smooth,\n",
    "‘g-’, linewidth=2, label=‘Smooth Case’)“,”ax1.set_xlabel(‘Number of\n",
    "Iterations (T)’)“,”ax1.set_ylabel(‘Stability Bound\n",
    "γ(m)’)“,”ax1.set_title(f’Comparison of Cases (m =\n",
    "{m})‘)“,”ax1.legend()“,”ax1.grid(True, alpha=0.3)“,”“,”\\# Right plot:\n",
    "Fixed T, varying m“,”T_fixed = 10000“,”m_values = np.logspace(2, 5,\n",
    "100)“,”“,”gamma_sc_m = compute_strongly_convex_bound(T_fixed, m_values,\n",
    "1.0, L, alpha)“,”gamma_gen_m = compute_general_bound(T_fixed, m_values,\n",
    "0.01, L)“,”gamma_smooth_m = compute_smooth_bound(T_fixed, m_values,\n",
    "eta_smooth, L, beta)“,”“,”ax2.loglog(m_values, gamma_sc_m, ’b-’,\n",
    "linewidth=2, label=‘Strongly Convex’)“,”ax2.loglog(m_values,\n",
    "gamma_gen_m, ‘r-’, linewidth=2, label=‘General\n",
    "Case’)“,”ax2.loglog(m_values, gamma_smooth_m, ‘g-’, linewidth=2,\n",
    "label=‘Smooth Case’)“,”ax2.set_xlabel(‘Sample Size\n",
    "(m)’)“,”ax2.set_ylabel(‘Stability Bound γ(m)’)“,”ax2.set_title(f’Sample\n",
    "Size Dependence (T = {T_fixed:,})’)“,”ax2.legend()“,”ax2.grid(True,\n",
    "alpha=0.3)“,”“,”plt.tight_layout()“,”plt.show()” \\] }, { “cell_type”:\n",
    "“markdown”, “metadata”: {}, “source”: \\[ “\\## 4. The Price of\n",
    "Stability”, “”, “The fundamental trade-off between optimization and\n",
    "generalization:”, “- **Optimization**: T = O(1/ε) iterations”, “-\n",
    "**Generalization**: T = O(1/ε²) iterations” \\] }, { “cell_type”: “code”,\n",
    "“execution_count”: null, “metadata”: {}, “outputs”: \\[\\], “source”: \\[\n",
    "“\\# Analyze price of stability for different error targets”, “epsilons =\n",
    "np.logspace(-3, -1, 20) \\# From 0.001 to 0.1”, “results =\n",
    "\\[analyze_price_of_stability(eps, L, alpha) for eps in epsilons\\]”, “”,\n",
    "“opt_iters = \\[r\\[‘optimization_iterations’\\] for r in results\\]”,\n",
    "“gen_iters = \\[r\\[‘generalization_iterations’\\] for r in results\\]”,\n",
    "“price_ratios = \\[r\\[‘price_ratio’\\] for r in results\\]”, “”, “\\# Create\n",
    "visualization”, “fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))”,\n",
    "“”, “\\# Left: Iterations vs error”, “ax1.loglog(epsilons, opt_iters,\n",
    "‘b-o’, linewidth=2, markersize=8, ”, ” label=‘Optimization (T =\n",
    "O(1/ε))’)“,”ax1.loglog(epsilons, gen_iters, ‘r-s’, linewidth=2,\n",
    "markersize=8, “,” label=‘Generalization (T =\n",
    "O(1/ε²))’)“,”ax1.loglog(epsilons, 1/epsilons, ‘b–’, alpha=0.5,\n",
    "label=‘1/ε reference’)“,”ax1.loglog(epsilons, 1/epsilons\\*\\*2, ‘r–’,\n",
    "alpha=0.5, label=‘1/ε² reference’)“,”ax1.set_xlabel(‘Target Error\n",
    "(ε)’)“,”ax1.set_ylabel(‘Required Iterations’)“,”ax1.set_title(‘The Price\n",
    "of Stability: Optimization vs\n",
    "Generalization’)“,”ax1.legend()“,”ax1.grid(True,\n",
    "alpha=0.3)“,”ax1.invert_xaxis()“,”“,”\\# Right: Price\n",
    "ratio“,”ax2.semilogx(epsilons, price_ratios, ‘g-^’, linewidth=2,\n",
    "markersize=8)“,”ax2.set_xlabel(‘Target Error\n",
    "(ε)’)“,”ax2.set_ylabel(‘Price Ratio (Gen. Iters / Opt.\n",
    "Iters)’)“,”ax2.set_title(‘How Much More Expensive is\n",
    "Generalization?’)“,”ax2.grid(True,\n",
    "alpha=0.3)“,”ax2.invert_xaxis()“,”“,”plt.tight_layout()“,”plt.show()“,”“,”\\#\n",
    "Print specific examples“,”print(\"\\nPrice of Stability\n",
    "Examples:\")“,”print(\"-\" \\* 60)“,”print(f\"{‘ε’:\\>10} \\| {‘Opt.\n",
    "Iters’:\\>12} \\| {‘Gen. Iters’:\\>12} \\| {‘Price\n",
    "Ratio’:\\>12}\")“,”print(\"-\" \\* 60)“,”for eps, opt, gen, ratio in\n",
    "zip(\\[0.1, 0.01, 0.001\\], “,” \\[opt_iters\\[0\\], opt_iters\\[10\\],\n",
    "opt_iters\\[-1\\]\\],“,” \\[gen_iters\\[0\\], gen_iters\\[10\\],\n",
    "gen_iters\\[-1\\]\\],“,” \\[price_ratios\\[0\\], price_ratios\\[10\\],\n",
    "price_ratios\\[-1\\]\\]):“,” print(f\"{eps:\\>10.3f} \\| {opt:\\>12,} \\|\n",
    "{gen:\\>12,} \\| {ratio:\\>12.1f}x\")” \\] }, { “cell_type”: “markdown”,\n",
    "“metadata”: {}, “source”: \\[ “\\## 5. Time-Varying Step Sizes”, “”,\n",
    "“Analyze stability with different learning rate schedules.” \\] }, {\n",
    "“cell_type”: “code”, “execution_count”: null, “metadata”: {}, “outputs”:\n",
    "\\[\\], “source”: \\[ “\\# Define different step size schedules”, “T =\n",
    "1000”, “t_values = np.arange(1, T+1)”, “”, “schedules = {”, ”\n",
    "‘Constant’: lambda t: 0.01 \\* np.ones_like(t),“,” ‘1/t’: lambda t: 0.1 /\n",
    "t,“,” ‘1/√t’: lambda t: 0.1 / np.sqrt(t),“,” ‘Exponential’: lambda t:\n",
    "0.1 \\* 0.99**t“,”}“,”“,”\\# Visualize schedules“,”plt.figure(figsize=(12,\n",
    "8))“,”“,”for name, schedule in schedules.items():“,” eta_values =\n",
    "schedule(t_values)“,” plt.semilogy(t_values, eta_values, linewidth=2,\n",
    "label=name)“,” “,” \\# Compute sum of squares“,” sum_eta_sq =\n",
    "np.sum(eta_values**2)“,” print(f\"{name:15} - Σ η(t)² =\n",
    "{sum_eta_sq:.6f}\")“,”“,”plt.xlabel(‘Iteration\n",
    "(t)’)“,”plt.ylabel(‘Learning Rate η(t)’)“,”plt.title(‘Different Learning\n",
    "Rate Schedules’)“,”plt.legend()“,”plt.grid(True,\n",
    "alpha=0.3)“,”plt.show()“,”“,”\\# Stability\n",
    "implications“,”print(\"\\nStability implications:\")“,”print(\"- Constant:\n",
    "Good for optimization, but Σ η² grows linearly\")“,”print(\"- 1/t:\n",
    "Guarantees convergence, Σ η² = O(log T)\")“,”print(\"- 1/√t: Balance\n",
    "between speed and stability\")“,”print(\"- Exponential: Fast decay, very\n",
    "stable but slow convergence\")” \\] }, { “cell_type”: “markdown”,\n",
    "“metadata”: {}, “source”: \\[ “\\## Key Takeaways”, “”, “1. **Strongly\n",
    "Convex Case**: Best stability properties with γ(m) = O(1/√T)”, “2.\n",
    "**General Case**: Requires careful learning rate tuning, γ(m) = O(√T)\n",
    "with fixed η”, “3. **Smooth Case**: Linear growth in T but better\n",
    "constants”, “4. **Price of Stability**: Generalization requires O(1/ε²)\n",
    "iterations vs O(1/ε) for optimization”, “5. **Learning Rate Schedules**:\n",
    "Trade-off between convergence speed and stability” \\] } \\], “metadata”:\n",
    "{ “kernelspec”: { “display_name”: “Python 3”, “language”: “python”,\n",
    "“name”: “python3” }, “language_info”: { “codemirror_mode”: { “name”:\n",
    "“ipython”, “version”: 3 }, “file_extension”: “.py”, “mimetype”:\n",
    "“text/x-python”, “name”: “python”, “nbconvert_exporter”: “python”,\n",
    "“pygments_lexer”: “ipython3”, “version”: “3.8.0” } }, “nbformat”: 4,\n",
    "“nbformat_minor”: 4 }"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
